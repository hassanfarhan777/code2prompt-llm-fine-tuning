{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddcfb9f",
   "metadata": {},
   "source": [
    "\n",
    "# Code-to-Prompt Semantic Matching Experiment (Manim CodeGen)\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Loading a Manim code-generation dataset from Hugging Face Datasets.\n",
    "- Using OpenAI's GPT model to match a code snippet to its most semantically similar prompt.\n",
    "- Validating and displaying the results.\n",
    "\n",
    "**Instructions:**  \n",
    "Set your OpenAI API key file path and other parameters in the configuration section below.\n",
    "\n",
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb84c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import openai\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from typing import List, Optional\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66897ef7",
   "metadata": {},
   "source": [
    "### Configuration Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232e9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these variables as needed\n",
    "DATASET_NAME = \"generaleoley/manim-codegen\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "REFERENCE_INDEX = 1          # Index of code example to test\n",
    "SUBSET_START = 900           # Start index for candidate prompt subset\n",
    "SUBSET_END = 1000            # End index (exclusive)\n",
    "OPENAI_API_KEY_PATH = \"open_ai_API.txt\"\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "OPENAI_MAX_TOKENS = 150\n",
    "OPENAI_TEMPERATURE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b30c7b",
   "metadata": {},
   "source": [
    "### 2. Environment & Resource Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39f391cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current device ID: 0\n",
      "Current device name: NVIDIA TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "def check_cuda_and_device():\n",
    "    \"\"\"Check and display CUDA/GPU availability.\"\"\"\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        device_id = torch.cuda.current_device()\n",
    "        print(f\"Current device ID: {device_id}\")\n",
    "        print(f\"Current device name: {torch.cuda.get_device_name(device_id)}\")\n",
    "\n",
    "check_cuda_and_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746320b",
   "metadata": {},
   "source": [
    "### 3. API Key Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efbcbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_key(path: str) -> str:\n",
    "    \"\"\"Reads the OpenAI API key from a file.\"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        raise RuntimeError(f\"API key file '{path}' not found.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading API key: {e}\")\n",
    "\n",
    "openai.api_key = load_api_key(OPENAI_API_KEY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5024da",
   "metadata": {},
   "source": [
    "### 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd260bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... (may take a moment)\n",
      "Dataset loaded: generaleoley/manim-codegen, split: train\n",
      "Total samples in split: 1622\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset... (may take a moment)\")\n",
    "data = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
    "print(f\"Dataset loaded: {DATASET_NAME}, split: {DATASET_SPLIT}\")\n",
    "print(f\"Total samples in split: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35dced",
   "metadata": {},
   "source": [
    "### 5. Select Reference Code and Candidate Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaad99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = REFERENCE_INDEX\n",
    "code_string = data[reference]['answer']\n",
    "\n",
    "print(\"\\n--- Reference Code Snippet ---\\n\")\n",
    "print(code_string)\n",
    "\n",
    "subset = data.select(range(SUBSET_START, SUBSET_END))\n",
    "print(f\"\\nSubset selected: [{SUBSET_START}, {SUBSET_END}) ({len(subset)} prompts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b50f4",
   "metadata": {},
   "source": [
    "### 6. Prepare LLM Prompt for Semantic Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_comparison_prompt(code: str, subset_examples) -> str:\n",
    "    \"\"\"\n",
    "    Builds a prompt asking the LLM to select the most similar query to the code from the subset.\n",
    "    \"\"\"\n",
    "    content = f\"The following code is provided:\\n\\n{code}\\n\\n\"\n",
    "    content += (\n",
    "        \"Please identify which query in the list below is the most similar to the code in terms of purpose. The options are:\\n\\n\"\n",
    "    )\n",
    "    for i, example in enumerate(subset_examples):\n",
    "        content += f\"Query {i+1}: {example['query']}\\n\"\n",
    "    return content\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": build_comparison_prompt(code_string, subset),\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860fcdf",
   "metadata": {},
   "source": [
    "### 7. Call OpenAI API to Find Most Similar Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf81ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_model(messages: List[dict], model: str, max_tokens: int, temperature: float) -> str:\n",
    "    \"\"\"\n",
    "    Calls the OpenAI API and returns the model's response.\n",
    "    \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"\\nQuerying OpenAI model for semantic match...\")\n",
    "open_gen = call_openai_model(messages, OPENAI_MODEL, OPENAI_MAX_TOKENS, OPENAI_TEMPERATURE)\n",
    "\n",
    "print(\"\\n--- LLM Response ---\\n\")\n",
    "print(open_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db13982",
   "metadata": {},
   "source": [
    "### 8. Extract Query Index from LLM Response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1af10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_number(response_text: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Extracts the query index number from LLM response text.\n",
    "    Returns None if not found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'Query (\\d+)', response_text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "query_number = extract_query_number(open_gen)\n",
    "if query_number is not None:\n",
    "    print(f\"\\nThe closest query (in subset) is: Query {query_number}\")\n",
    "else:\n",
    "    print(\"\\nNo query number found in the response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad8ccc",
   "metadata": {},
   "source": [
    "\n",
    "### 9. Display Matched Query and Find Its Position in Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e91cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current device ID: 0\n",
      "Current device name: NVIDIA TITAN Xp\n",
      "Loading dataset... (may take a moment)\n",
      "Dataset loaded: generaleoley/manim-codegen, split: train\n",
      "Total samples in split: 1622\n",
      "\n",
      "--- Reference Code Snippet ---\n",
      "\n",
      "\\n class Main(Scene):\n",
      "    def construct(self):\n",
      "            self.play(Transform(text,Text(\"animate.flip\").shift(UP*2.5)), run_time=0.5)\n",
      "            triangle = Triangle()\n",
      "            self.play(triangle.animate.flip())\n",
      "            self.remove(triangle)\n",
      "\n",
      "Subset selected: [900, 1000) (100 prompts)\n",
      "\n",
      "Querying OpenAI model for semantic match...\n",
      "\n",
      "--- LLM Response ---\n",
      "\n",
      "The code you provided is focused on creating a simple animation using the Manim library, specifically demonstrating a transformation effect (flipping a triangle) and a text transformation. The most similar query in terms of purpose would be one that also involves creating an animation with a transformation effect.\n",
      "\n",
      "Among the provided queries, **Query 91** is the most similar to your code. It involves moving a piece of text (\"animate.flip\") to the top of the screen and then animating a triangle to flip over before disappearing. This aligns closely with the purpose of your code, which also involves a text transformation followed by a geometric shape (triangle) performing a flip animation.\n",
      "\n",
      "Hereâ€™s a brief comparison:\n",
      "- **Your Code**: Transforms text and animates a\n",
      "\n",
      "The closest query (in subset) is: Query 91\n",
      "\n",
      "--- Matched Query from Subset ---\n",
      "\n",
      "I'd like to create an animation where a piece of text saying \"animate.flip\" moves and settles at the top of the screen, and then a triangle appears, flips over, and then disappears. Can this be done quickly, maybe in half a second?\n",
      "\n",
      "Reference paragraph found at index position: 990 for reference: 1\n",
      "\n",
      "--- Full Prompt from Dataset ---\n",
      "\n",
      "('I\\'d like to create an animation where a piece of text saying \"animate.flip\" '\n",
      " 'moves and settles at the top of the screen, and then a triangle appears, '\n",
      " 'flips over, and then disappears. Can this be done quickly, maybe in half a '\n",
      " 'second?')\n"
     ]
    }
   ],
   "source": [
    "if query_number is not None:\n",
    "    matched_query_text = subset[query_number - 1]['query']\n",
    "    print(\"\\n--- Matched Query from Subset ---\\n\")\n",
    "    print(matched_query_text)\n",
    "\n",
    "    # Find its original index in the full dataset\n",
    "    def find_paragraph_index(paragraphs: List[str], reference_paragraph: str) -> Optional[int]:\n",
    "        \"\"\"Finds the index of a reference paragraph in a list.\"\"\"\n",
    "        try:\n",
    "            return paragraphs.index(reference_paragraph)\n",
    "        except ValueError:\n",
    "            print(\"Reference paragraph not found in the list.\")\n",
    "            return None\n",
    "\n",
    "    all_queries = data['query']\n",
    "    index_position = find_paragraph_index(all_queries, matched_query_text)\n",
    "    if index_position is not None:\n",
    "        print(f\"\\nReference paragraph found at index position: {index_position} for reference: {reference}\")\n",
    "        print(\"\\n--- Full Prompt from Dataset ---\\n\")\n",
    "        pprint.pprint(data[index_position]['query'])\n",
    "else:\n",
    "    print(\"Could not match query; please review LLM output.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649fa40",
   "metadata": {},
   "source": [
    "### 10. Conclusion & Next Steps\n",
    "\n",
    "- The notebook demonstrated code-to-prompt semantic matching using LLMs.\n",
    "- You can change the `REFERENCE_INDEX` or `SUBSET_START/END` at the top to experiment with different examples or candidate pools.\n",
    "\n",
    " **For further exploration:**  \n",
    " - Batch process multiple code snippets for evaluation metrics.\n",
    " - Use sentence embeddings (e.g., with `SentenceTransformer`) for additional semantic similarity scoring.\n",
    " - Visualize results across many examples to benchmark LLM performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866a20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e4904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "found_mod_lec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
